# Mini Projects

This repository is a collection of small DNN experiments created for learning and practice. Each experiment focuses on understanding a specific concept such as data loading, model training, evaluation, and experimentation with neural networks using simple datasets.

1. [Rice Classification](rice_type_classification.ipynb) – It demonstrates a complete **binary classification pipeline using PyTorch** on tabular data, covering **data preprocessing, custom Dataset and DataLoader creation**, and **neural network modeling**.It highlights hands-on usage of **model training loops, loss optimization (BCELoss + Adam), validation/testing workflows**, and **performance visualization** through accuracy and loss curves.

2. [Breast Canser Classification](breast_cancer_classification.ipynb) - This mini-project demonstrates an end-to-end binary classification pipeline on tabular medical data using PyTorch. It covers dataset acquisition from Kaggle, exploratory preprocessing, label encoding (malignant vs benign), feature normalization, and train/validation/test splitting. The project implements a custom PyTorch Dataset and DataLoader, followed by a fully connected neural network with multiple hidden layers and sigmoid output for probabilistic prediction. It highlights hands-on training loops with BCELoss and Adam optimizer, device-aware (CPU/GPU) execution, and systematic evaluation using accuracy on validation and test sets. Model performance is analyzed through loss and accuracy curves, providing clear insights into convergence behavior and generalization on real-world clinical data.

3. [Moons Dataset Classification] (moons_binary_classification.ipynb) – This mini-project demonstrates a clean binary classification experiment on a synthetic non-linearly separable dataset (two-moons) generated using sklearn.datasets.make_moons. It covers creating a toy dataset, packaging it into a Pandas DataFrame, performing simple feature scaling/normalization, and building a custom PyTorch Dataset + DataLoader for mini-batch training. The model is a small MLP (fully connected network) with ReLU activations and a sigmoid output layer, trained using a standard training loop with BCELoss and Adam optimizer on CPU/GPU. The experiment emphasizes how multi-layer perceptrons learn non-linear decision boundaries, and visualizes training behavior through a loss vs epochs curve for quick convergence inspection.
